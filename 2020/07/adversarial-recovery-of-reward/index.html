<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="generator" content="Hugo 0.69.0" />
  <link rel="canonical" href="https://dennislblog.github.io/cnblog/2020/07/adversarial-recovery-of-reward/">

  

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" href="https://dennislblog.github.io/cnblog/css/prism.css" media="none" onload="this.media='all';">
  <link rel="stylesheet" href="https://dennislblog.github.io/cnblog/css/syntax.css" media="none" onload="this.media='all';">


  
  
  <link rel="stylesheet" type="text/css" href="https://dennislblog.github.io/cnblog/css/styles.css">

  <style id="inverter" media="none">
    .intro-and-nav, .main-and-footer { filter: invert(100%) }
    * { background-color: inherit }
    img:not([src*=".svg"]), .colors, iframe, .demo-container { filter: invert(100%) }
  </style>

  
  
  <title>Adversarial Reward Learning from Latent Space of LOB | My Site Title</title>
  

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"]},
    },
    "HTML-CSS": {
    	scale: 80,
    	styles: {
    		".MathJax": {color: "rgb(91, 58, 17)",}
    	},
    },
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>




</head>

  <body>
    <a href="#main">skip to content</a>
    <svg style="display: none">
  <symbol id="bookmark" viewBox="0 0 40 50">
   <g transform="translate(2266 3206.2)">
    <path style="stroke:currentColor;stroke-width:3.2637;fill:none" d="m-2262.2-3203.4-.2331 42.195 16.319-16.318 16.318 16.318.2331-42.428z"/>
   </g>
  </symbol>

  <symbol id="w3c" viewBox="0 0 127.09899 67.763">
   <text font-size="83" style="font-size:83px;font-family:Trebuchet;letter-spacing:-12;fill-opacity:0" letter-spacing="-12" y="67.609352" x="-26.782778">W3C</text>
   <text font-size="83" style="font-size:83px;font-weight:bold;font-family:Trebuchet;fill-opacity:0" y="67.609352" x="153.21722" font-weight="bold">SVG</text>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m33.695.377 12.062 41.016 12.067-41.016h8.731l-19.968 67.386h-.831l-12.48-41.759-12.479 41.759h-.832l-19.965-67.386h8.736l12.061 41.016 8.154-27.618-3.993-13.397h8.737z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m91.355 46.132c0 6.104-1.624 11.234-4.862 15.394-3.248 4.158-7.45 6.237-12.607 6.237-3.882 0-7.263-1.238-10.148-3.702-2.885-2.47-5.02-5.812-6.406-10.022l6.82-2.829c1.001 2.552 2.317 4.562 3.953 6.028 1.636 1.469 3.56 2.207 5.781 2.207 2.329 0 4.3-1.306 5.909-3.911 1.609-2.606 2.411-5.738 2.411-9.401 0-4.049-.861-7.179-2.582-9.399-1.995-2.604-5.129-3.912-9.397-3.912h-3.327v-3.991l11.646-20.133h-14.062l-3.911 6.655h-2.493v-14.976h32.441v4.075l-12.31 21.217c4.324 1.385 7.596 3.911 9.815 7.571 2.22 3.659 3.329 7.953 3.329 12.892z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.21 0 1.414 8.6-5.008 9.583s-1.924-4.064-5.117-6.314c-2.693-1.899-4.447-2.309-7.186-1.746-3.527.73-7.516 4.938-9.258 10.13-2.084 6.21-2.104 9.218-2.178 11.978-.115 4.428.58 7.043.58 7.043s-3.04-5.626-3.011-13.866c.018-5.882.947-11.218 3.666-16.479 2.404-4.627 5.954-7.404 9.114-7.728 3.264-.343 5.848 1.229 7.841 2.938 2.089 1.788 4.213 5.698 4.213 5.698l4.94-9.837z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.82 48.674s-2.208 3.957-3.589 5.48c-1.379 1.524-3.849 4.209-6.896 5.555-3.049 1.343-4.646 1.598-7.661 1.306-3.01-.29-5.807-2.032-6.786-2.764-.979-.722-3.486-2.864-4.897-4.854-1.42-2-3.634-5.995-3.634-5.995s1.233 4.001 2.007 5.699c.442.977 1.81 3.965 3.749 6.572 1.805 2.425 5.315 6.604 10.652 7.545 5.336.945 9.002-1.449 9.907-2.031.907-.578 2.819-2.178 4.032-3.475 1.264-1.351 2.459-3.079 3.116-4.108.487-.758 1.276-2.286 1.276-2.286l-1.276-6.644z"/>
  </symbol>

  <symbol id="tag" viewBox="0 0 177.16535 177.16535">
    <g transform="translate(0 -875.2)">
     <path style="fill-rule:evenodd;stroke-width:0;fill:currentColor" d="m159.9 894.3-68.79 8.5872-75.42 77.336 61.931 60.397 75.429-76.565 6.8495-69.755zm-31.412 31.835a10.813 10.813 0 0 1 1.8443 2.247 10.813 10.813 0 0 1 -3.5174 14.872l-.0445.0275a10.813 10.813 0 0 1 -14.86 -3.5714 10.813 10.813 0 0 1 3.5563 -14.863 10.813 10.813 0 0 1 13.022 1.2884z"/>
    </g>
  </symbol>

  <symbol id="balloon" viewBox="0 0 141.73228 177.16535">
   <g transform="translate(0 -875.2)">
    <g>
     <path style="fill:currentColor" d="m68.156 882.83-.88753 1.4269c-4.9564 7.9666-6.3764 17.321-5.6731 37.378.36584 10.437 1.1246 23.51 1.6874 29.062.38895 3.8372 3.8278 32.454 4.6105 38.459 4.6694-.24176 9.2946.2879 14.377 1.481 1.2359-3.2937 5.2496-13.088 8.886-21.623 6.249-14.668 8.4128-21.264 10.253-31.252 1.2464-6.7626 1.6341-12.156 1.4204-19.764-.36325-12.93-2.1234-19.487-6.9377-25.843-2.0833-2.7507-6.9865-7.6112-7.9127-7.8436-.79716-.20019-6.6946-1.0922-6.7755-1.0248-.02213.0182-5.0006-.41858-7.5248-.22808l-2.149-.22808h-3.3738z"/>
     <path style="fill:currentColor" d="m61.915 883.28-3.2484.4497c-1.7863.24724-3.5182.53481-3.8494.63994-2.4751.33811-4.7267.86957-6.7777 1.5696-.28598 0-1.0254.20146-2.3695.58589-5.0418 1.4418-6.6374 2.2604-8.2567 4.2364-6.281 7.6657-11.457 18.43-12.932 26.891-1.4667 8.4111.71353 22.583 5.0764 32.996 3.8064 9.0852 13.569 25.149 22.801 37.517 1.3741 1.841 2.1708 2.9286 2.4712 3.5792 3.5437-1.1699 6.8496-1.9336 10.082-2.3263-1.3569-5.7831-4.6968-21.86-6.8361-33.002-.92884-4.8368-2.4692-14.322-3.2452-19.991-.68557-5.0083-.77707-6.9534-.74159-15.791.04316-10.803.41822-16.162 1.5026-21.503 1.4593-5.9026 3.3494-11.077 6.3247-15.852z"/>
     <path style="fill:currentColor" d="m94.499 885.78c-.10214-.0109-.13691 0-.0907.0409.16033.13489 1.329 1.0675 2.5976 2.0723 6.7003 5.307 11.273 14.568 12.658 25.638.52519 4.1949.24765 14.361-.5059 18.523-2.4775 13.684-9.7807 32.345-20.944 53.519l-3.0559 5.7971c2.8082.76579 5.7915 1.727 8.9926 2.8441 11.562-11.691 18.349-19.678 24.129-28.394 7.8992-11.913 11.132-20.234 12.24-31.518.98442-10.02-1.5579-20.876-6.7799-28.959-.2758-.4269-.57803-.86856-.89617-1.3166-3.247-6.13-9.752-12.053-21.264-16.131-2.3687-.86369-6.3657-2.0433-7.0802-2.1166z"/>
     <path style="fill:currentColor" d="m32.52 892.22c-.20090-.13016-1.4606.81389-3.9132 2.7457-11.486 9.0476-17.632 24.186-16.078 39.61.79699 7.9138 2.4066 13.505 5.9184 20.562 5.8577 11.77 14.749 23.219 30.087 38.74.05838.059.12188.1244.18052.1838 1.3166-.5556 2.5965-1.0618 3.8429-1.5199-.66408-.32448-1.4608-1.3297-3.8116-4.4602-5.0951-6.785-8.7512-11.962-13.051-18.486-5.1379-7.7948-5.0097-7.5894-8.0586-13.054-6.2097-11.13-8.2674-17.725-8.6014-27.563-.21552-6.3494.13041-9.2733 1.775-14.987 2.1832-7.5849 3.9273-10.986 9.2693-18.07 1.7839-2.3656 2.6418-3.57 2.4409-3.7003z"/>
     <path style="fill:currentColor" d="m69.133 992.37c-6.2405.0309-12.635.76718-19.554 2.5706 4.6956 4.7759 9.935 10.258 12.05 12.625l4.1272 4.6202h11.493l3.964-4.4516c2.0962-2.3541 7.4804-7.9845 12.201-12.768-8.378-1.4975-16.207-2.6353-24.281-2.5955z"/>
     <rect style="stroke-width:0;fill:currentColor" ry="2.0328" height="27.746" width="22.766" y="1017.7" x="60.201"/>
    </g>
   </g>
  </symbol>

  <symbol id="info" viewBox="0 0 41.667 41.667">
   <g transform="translate(-37.035 -1004.6)">
    <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m76.25 1030.2a18.968 18.968 0 0 1 -23.037 13.709 18.968 18.968 0 0 1 -13.738 -23.019 18.968 18.968 0 0 1 23.001 -13.768 18.968 18.968 0 0 1 13.798 22.984"/>
    <g transform="matrix(1.1146 0 0 1.1146 -26.276 -124.92)">
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m75.491 1039.5v-8.7472"/>
     <path style="stroke-width:0;fill:currentColor" transform="scale(-1)" d="m-73.193-1024.5a2.3719 2.3719 0 0 1 -2.8807 1.7142 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
   </g>
  </symbol>

  <symbol id="warning" viewBox="0 0 48.430474 41.646302">
    <g transform="translate(-1.1273 -1010.2)">
     <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:4.151;fill:none" d="m25.343 1012.3-22.14 37.496h44.28z"/>
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:4.1512;fill:none" d="m25.54 1027.7v8.7472"/>
     <path style="stroke-width:0;fill:currentColor" d="m27.839 1042.8a2.3719 2.3719 0 0 1 -2.8807 1.7143 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
  </symbol>

  <symbol id="menu" viewBox="0 0 50 50">
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="0" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="20" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="40" x="0"/>
   </symbol>

   <symbol id="link" viewBox="0 0 50 50">
    <g transform="translate(0 -1002.4)">
     <g transform="matrix(.095670 0 0 .095670 2.3233 1004.9)">
      <g>
       <path style="stroke-width:0;fill:currentColor" d="m452.84 192.9-128.65 128.65c-35.535 35.54-93.108 35.54-128.65 0l-42.881-42.886 42.881-42.876 42.884 42.876c11.845 11.822 31.064 11.846 42.886 0l128.64-128.64c11.816-11.831 11.816-31.066 0-42.9l-42.881-42.881c-11.822-11.814-31.064-11.814-42.887 0l-45.928 45.936c-21.292-12.531-45.491-17.905-69.449-16.291l72.501-72.526c35.535-35.521 93.136-35.521 128.64 0l42.886 42.881c35.535 35.523 35.535 93.141-.001 128.66zm-254.28 168.51-45.903 45.9c-11.845 11.846-31.064 11.817-42.881 0l-42.884-42.881c-11.845-11.821-11.845-31.041 0-42.886l128.65-128.65c11.819-11.814 31.069-11.814 42.884 0l42.886 42.886 42.876-42.886-42.876-42.881c-35.54-35.521-93.113-35.521-128.65 0l-128.65 128.64c-35.538 35.545-35.538 93.146 0 128.65l42.883 42.882c35.51 35.54 93.11 35.54 128.65 0l72.496-72.499c-23.956 1.597-48.092-3.784-69.474-16.283z"/>
      </g>
     </g>
    </g>
  </symbol>

  <symbol id="doc" viewBox="0 0 35 45">
   <g transform="translate(-147.53 -539.83)">
    <path style="stroke:currentColor;stroke-width:2.4501;fill:none" d="m149.38 542.67v39.194h31.354v-39.194z"/>
    <g style="stroke-width:25" transform="matrix(.098003 0 0 .098003 133.69 525.96)">
     <path d="m220 252.36h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path style="stroke:currentColor;stroke-width:25;fill:none" d="m220 409.95h200"/>
     <path d="m220 488.74h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path d="m220 331.15h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
    </g>
   </g>
 </symbol>

 <symbol id="tick" viewBox="0 0 177.16535 177.16535">
  <g transform="translate(0 -875.2)">
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="155" width="40" y="702.99" x="556.82"/>
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="40" width="90.404" y="817.99" x="506.42"/>
  </g>
 </symbol>
</svg>

    <div class="wrapper">
      <header class="intro-and-nav" role="banner">
  <div>
    <div class="intro">
      <a class="logo" href="/" aria-label="My Site Title home page">
        <img src="https://dennislblog.github.io/cnblog/pics/logo.svg" alt="">
      </a>
      <p class="library-desc">
        
        Hello, you all
        
      </p>
    </div>
    <nav id="patterns-nav" class="patterns" role="navigation">
  <h2 class="vh">Main navigation</h2>
  <button id="menu-button" aria-expanded="false">
    <svg viewBox="0 0 50 50" aria-hidden="true" focusable="false">
      <use xlink:href="#menu"></use>
    </svg>
    Menu
  </button>
  
  <ul id="patterns-list">
  
    <li class="pattern">
      
      
      
      
      <a href="/cnblog/post/" aria-current="page">
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Blog</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/cnblog/tags/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Tags</span>
      </a>
    </li>
  
  </ul>
</nav>
    
    



  <hr>
  <nav class="patterns" aria-labelledby="toc-heading">
    <h4 id="toc-heading" style="margin-bottom: 1em">Table of contents</h4> 
    <ol>
      
        
        <li class="toc-h2">
          
          
          
          
          <a href="#summary" style="padding-left: 0rem">
            Summary
          </a>
        </li>
      
        
        <li class="toc-h2">
          
          
          
          
          <a href="#%e8%83%8c%e6%99%af" style="padding-left: 0rem">
            背景
          </a>
        </li>
      
        
        <li class="toc-h2">
          
          
          
          
          <a href="#reference" style="padding-left: 0rem">
            Reference
          </a>
        </li>
      
    </ol>
  </nav>





    
  </div>
</header>
      <div class="main-and-footer">
        <div>
          
  <main id="main">
    <h1>
      <svg class="bookmark-icon" aria-hidden="true" viewBox="0 0 40 50" focusable="false">
        <use xlink:href="#bookmark"></use>
      </svg>
      Adversarial Reward Learning from Latent Space of LOB
    </h1>

    <div class="date">
      
      
      <strong aria-hidden="true">Publish date: </strong>Wednesday, July 22, 2020
      
        
      
    </div>

    
      <div class="tags">
        <strong aria-hidden="true">Tags: </strong>
        <ul aria-label="tags">
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              
              <a href="https://dennislblog.github.io/cnblog/tags/review/">Review</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              
              <a href="https://dennislblog.github.io/cnblog/tags/irl/">IRL</a>
            </li>
          
        </ul>
      </div>
    
    <p>summaries on <a href="https://arxiv.org/abs/1912.04242v1">Adversarial recovery of agent rewards from latent spaces of the limit order book</a>(Roa-Vicens &amp; al., 2019)</p>
<h2 id="summary">Summary</h2>
<ol>
<li>金融问题通常 non-stationary 利用对抗学习来适应复杂多变的环境，以及是否可以迁移到新的环境</li>
<li>先对真实交易数据LOB抽象化<code>Mixture-Density Recurrent Network (RNN-MDN) architecture</code> 然后训练3个强化学习<code>model-based? agent: A2C, DQN, PG</code> 把他们的<code>state-action trajectories</code>当做逆强化学习的输入，最后测试&hellip;.</li>
</ol>
<hr>


<aside aria-label="note" class="note">
  <div>
    <svg class="sign" aria-hidden="true" viewBox="0 0 41.667306 41.66729" focusable="false">
      <use xlink:href="#info"></use>
    </svg>
    
为什么不使用PNL作为奖励函数来学习交易策略 
    <br>
    强化学习在model-free的应用中，通过不断与环境交互，从随机行为慢慢习得能稳定获得高回报的策略，但是对于自动驾驶和市场交易这种探索成本较高的任务不适合，即使在训练集上表现得好也不意味着能在real-time测试过程中保证成功<code>non-stationary env</code>。金融数据往往是<code>high-dimensional, stochastic, non-stationary unknown transition dynamics</code>，于是这篇文章希望通过在真实交易数据中，分析一个专家的交易策略，来学习一个<code>robust reward function</code>，希望这个回报函数比手动设计的更能适应多变的环境
  </div>
</aside>



<aside aria-label="note" class="note">
  <div>
    <svg class="sign" aria-hidden="true" viewBox="0 0 41.667306 41.66729" focusable="false">
      <use xlink:href="#info"></use>
    </svg>
    
传统建模方法有什么问题？ 
    <br>
    在开发交易策略时，往往先通过统计建模(例如Auto Regression)来拟合时间序列，然后进行预测，基于模型给出的预测，结合个人风险偏好和启发性/专业知识来做出决策。然后在回测中检验效果。<code>Vector Auto-Regressive model (VAR)</code>可以拟合<code>the non-stationary aspect of financial time-series and can forecast with multiple variances</code>，但这些建模都需要满足一定的假设和对数据的理解，在<code>real time</code>运行中可能条件不满足，错误的统计假设可能导致<code>error compounding</code>&ndash; RL也有这个问题耶，这个和<code>manual labeled deep learning</code>有什么区别？在deep learning里，行为是不改变环境和后续奖励的
  </div>
</aside>

<h2 id="背景">背景</h2>
<p>[1] Hendricks, Dieter, et al. &ldquo;<a href="https://arxiv.org/pdf/1712.01137.pdf">Inferring agent objectives at different scales of a complex adaptive system</a>.&rdquo; arXiv preprint arXiv:1712.01137 (2017 NIPS).</p>


<blockquote class="blockquote">
  <p>
    <p><code>Investors</code> use their understanding of asset dynamics to time buying and selling decisions for financial gain, <code>traders</code> use their understanding of market dynamics to plan trades and minimize the cost of realising investment decisions and <code>market makers</code> use their understanding of investor demand to profit from liquidity provision.</p>
<p>The field of <code>market microstructure</code> studies the dynamics of price formation in this system at intraday time scales, considering how mechanistic rules, regulatory oversight and social behaviors of participants interact to manifest the observed time series</p>
<p>Given access to sample state-action space trajectories collected from observing an agent's behavior, the objective of <code>inverse reinforcement learning</code> is to find a reward function that induces agents to follow trajectories matching the expert trajectories.</p>
<p>The <code>motivation</code> is that if the behavior at a certain (cluster) scale resulted in a positive (negative) price return, this could indicate net buying (selling) decisions for that cluster. We will thus use the sign of average price return to assign the action to one of three states, {BUY, SELL, NEUTRAL}.</p>

    
  </p>
</blockquote>

<div class="expandable-section">
  
    <button aria-expanded="false" data-expands="js-expandable-5ae1f58986ca3486c867e0e7c9359690">
      <span class="expandable-label">Detail</span>
      <svg aria-hidden="true" focusable="false" viewBox="0 0 70.866142 70.866141">
        <g transform="translate(0 -981.5)">
          <rect style="stroke-width:0;fill:currentColor" ry="5" height="60" width="9.8985" y="987.36" x="30.051" class="up-strut" />
          <rect style="stroke-width:0;fill:currentColor" ry="5" height="10" width="60" y="1012.4" x="5"/>
        </g>
      </svg>
    </button>
  
  <div id="js-expandable-5ae1f58986ca3486c867e0e7c9359690" hidden>
    <ul>
<li>
<p>这篇文章以15年在<code>quant finance</code>上发表的<code>temporal clustering</code>为基础，为以后<code>hierarchical RL framework with multi-scale learning in finance</code>做探索研究</p>
<ul>
<li><code>time scale</code>可以简单的理解为窗口，比如<code>5-minute scale</code>，计算前后5分钟窗口变量的变化
$$\Delta f_t^{5 \min} = \frac{f_{t}^{5 \min}-f_{t-1}^{5 \min}}{f_{t-1}^{5 \min}}$$</li>
</ul>
</li>
<li>
<p>数据来自南非交易2012-11-01到2012-11-30的交易数据(Thomson Reuters Tick History)，变量包括<code>Change in trade price, quote spread, trade volume and quote volume imbalance</code>，计算<code>feature similarity</code>把所有state归纳成6个cluster(看图中数字)</p>
<p><img src="https://i.postimg.cc/m2hWRgPX/image.png" alt="experiment result" title="`negative spread` = 买卖成本减小，`negative imbalance` = 买需求减小，比如`cluster 1`的解释，好像是矛盾的|||80"></p>
</li>
</ul>

  </div>
</div>

<p>[2] Roa-Vicens, J., Chtourou, C., Filos, A., Rullan, F., Gal, Y., &amp; Silva, R. (2019). <a href="https://arxiv.org/pdf/1906.04813.pdf">Towards Inverse Reinforcement Learning for Limit Order Book Dynamics</a>. arXiv preprint arXiv:1906.04813.</p>


<blockquote class="blockquote">
  <p>
    <p>Multi-agent learning is a promising method to <code>simulate aggregate competitive</code> behaviors in finance. Learning expert agents' reward functions through their external demonstrations is hence particularly relevant for subsequent design of realistic agent-based simulations</p>
<p><code>Expert Agent</code> is unaware of the strategies of each of the other trading agents, but adapts to them through trial and error, as in reinforcement learning frameworks</p>

    
  </p>
</blockquote>

<div class="expandable-section">
  
    <button aria-expanded="false" data-expands="js-expandable-9731906aa0fd087431abe72254169c00">
      <span class="expandable-label">Detail</span>
      <svg aria-hidden="true" focusable="false" viewBox="0 0 70.866142 70.866141">
        <g transform="translate(0 -981.5)">
          <rect style="stroke-width:0;fill:currentColor" ry="5" height="60" width="9.8985" y="987.36" x="30.051" class="up-strut" />
          <rect style="stroke-width:0;fill:currentColor" ry="5" height="10" width="60" y="1012.4" x="5"/>
        </g>
      </svg>
    </button>
  
  <div id="js-expandable-9731906aa0fd087431abe72254169c00" hidden>
    <p><del>希望MM下一个阶段的limit order总是能正好匹配N个trader的market order，每一个阶段market order重新生成，但是和上一阶段没有match的订单有关系</del></p>
<ul>
<li>
<p>为什么<code>IRL</code>用于研究金融问题，因为环境<code>non-stationary</code>，<code>state-distribution</code>可能发生了改变，所以手动设置reward的假设可能在<code>real time test phase</code>不成立</p>
</li>
<li>
<p>游戏环境：$N$ traders and $1$ expert $I_\max$ inventory control</p>
<ul>
<li>观测变量 $\langle v_b(t), v_a(t), i(t)\rangle$ LOB上的best bid/ask volume以及自己的inventory level</li>
<li>行动变量 $v_b(t), v_a(t)$ 分别代表自己(专家)在bid/ask book上limit order的volume</li>
<li>状态转移 第$n$个trader，会提交$1$个market order，至于是buy or sell ~ Bernoulli($(f_b)^{-1}(f_a + f_b)$) where $f_b = \exp(\tau^{-1}(v_b(t-1)))$，如果上一轮bid还有很多没match，即$v_b(t-1) \nearrow$，那么这一轮就更有可能提交<code>market sell to match the bid. </code></li>
<li>奖励函数 <code>clear as much as possible</code> 如果你预计下一轮全<code>market buy</code>那么最好全部<code>limit sell</code>去match，得到最高奖励 $r(t) = N - v_b(t) - v_a(t)$，如果自己的inventory control $i(t) &gt; I_\max$ 则游戏结束</li>
</ul>
</li>
<li>
<p>所以这里面是有<code>ground truth reward function</code>的，他们是希望通过<code>IRL</code>去找到一个替代奖励，使得训练出来的策略和用<code>true reward</code>跑出来的策略成绩差不多<code> Expected Value Differences (EVD)</code>。他们的结论就是<code>max entropy</code>不能有效还原<code>non-linear reward function</code> ? 为什么，我得赶紧做一个IRL的总结</p>
<p><img src="https://i.postimg.cc/MT6vc75k/image.png" alt="experiment result" title="EVD在真实奖励线性和非线性条件下，基于不同IRL的策略相对于true reward最优策略的表现差异|||80"></p>
</li>
</ul>

  </div>
</div>

<p>[3] Wei, H., Wang, Y., Mangu, L., &amp; Decker, K. (2019). Model-based Reinforcement Learning for Predictions and Control for Limit Order Books. arXiv preprint arXiv:1910.03743.</p>


<blockquote class="blockquote">
  <p>
    <p>Kaiser et al. (2019) builds a neural network <code>simulated environment</code> that not only shares an action space and reward space with the original environment env but also produces observations in the same format</p>
<p>The <code>time series evolution of an LOB</code> can be seen as a 3-dimensional tensor: the first dimension represents <code>time</code>, the second dimension is <code>level</code>, and the third represents <code>prices and order quantities</code> on both the buy and sell sides(Gould et al. 2013).</p>
<p><code>Mid Price</code> is the mean value of the first-level ask and bid price. <code>Trade prints</code> are the record of executed trades that contain information of direction (buy or sell), trading price, and
quantity (e.g., 10 mkt buys @14$).</p>

    
  </p>
</blockquote>

<div class="expandable-section">
  
    <button aria-expanded="false" data-expands="js-expandable-4e1e1c3cce6f6188fa300350a23a4a1b">
      <span class="expandable-label">Detail</span>
      <svg aria-hidden="true" focusable="false" viewBox="0 0 70.866142 70.866141">
        <g transform="translate(0 -981.5)">
          <rect style="stroke-width:0;fill:currentColor" ry="5" height="60" width="9.8985" y="987.36" x="30.051" class="up-strut" />
          <rect style="stroke-width:0;fill:currentColor" ry="5" height="10" width="60" y="1012.4" x="5"/>
        </g>
      </svg>
    </button>
  
  <div id="js-expandable-4e1e1c3cce6f6188fa300350a23a4a1b" hidden>
    <p><del>奖励函数有点意思，配合将LOB抽象化，model-based transition，似乎确实可以转换为较优的交易策略</del></p>
<ul>
<li>游戏环境：
<ul>
<li>状态变量： $\langle f(ob_{t-T:t}), u_t, po_t \rangle$ 其中$f$代表<code>auto encoder</code> $ob_{t-T:t}$代表整个LOB(本文用了3层)，$u_t$ 代表发生在这个<code>time window</code>里的<code>sequence of trade point</code> 然后 $po_t$ 代表<code>agent's position at time t</code></li>
<li>行动变量：$a_t = \pm q$ 代表提交的market order 方向和数量，以<code>mid price</code>成交</li>
<li>状态转移：针对每一个发生的交易，看成一个transition: $\mathcal{T}(s_{t+1:t+T} | s_{t-T:t-1}, a_t)$ 这样做的问题在于忽略<code>order arrival frequency</code>的影响，其中$s_{t-T:t-1} = \langle f(ob_{t-T:t-1}), u_{t-T:t-1} \rangle$代表观测量和交易信息</li>
<li>奖励函数：<code>mark-to-market PnL</code> $R(t) = \Delta \text{mid}(t+1) \times po_t$ 感觉有点奇怪，只考虑了资产价格变化(不出售资产)，在Spooner, Thomas, et al., (2018) 的文章中，这部分叫做<code>change in cash holdings due to exposure to changes in price</code> 可是是$\Delta \text{mid}(t)$. 我不确定这个定义的合理性，有点像<code>greedy</code>算法，即如果下一个时刻价格会增加，则此时的position越大越好</li>
</ul>
</li>
<li>data: LOB更新频率0.17s, 腾讯这只股票61天作为train/validate，19天作为测试集，600万个transitions for train, 200万 for testing. 在剔除异常值(交易量大于1000小于100的)后，用min-max-normalization规范数据。然后reward用sigmoid规范(这么做的合理性？)
<ul>
<li>将$T=40 \times 4 \times L=3$的LOB数据提取出$1 \times 16$的抽象<code>vector</code>
<img src="auto_encode.png" alt="" title="T=40, L=3|||60"></li>
<li>State由上面CNN-AE的抽象向量和交易信息组成，假设$T=32$的时间内最多只能发生$10$次交易，如果不满10次，用$0$填充</li>
<li>Transition Model 用RNN-MDN 训练，$$p(s&rsquo;,s,a) = \sum_{k=1}^K w_k(s,a)\mathcal{D}(s'|\mu_k(s,a), \sigma_k^2(s,a))$$将<code>next state</code>的分布描述成若干个高斯叠加分布, State是一个<code>tuple</code> $(\mathbb{R}^{16}, \mathbb{R}^{10}, \mathbb{N}^{po_\max}), K=5$
<img src="rnn_mdn.png" alt="" title="N=10, K=5|||60"></li>
<li>Reward Model 就如同我所好奇的那样，当前状态下的<code>reward</code>取决于下一个状态的<code>mid-price</code>，即当前<code>position</code>在未来的价值（有点意思！！）所以作者需要预测下一个LOB observation（为啥不直接预测mid price?）我想是因为可以利用那个auto-encoder吧。<code>reward model</code>由128 LSTM units 和 40 Dense units 组成 $$r_t = \mathcal{R}(z_t, z_{t+1};\beta) \times po_t$$</li>
<li>Action: 将trade quantity and direction 分成 21 份 $\langle -1000, -900, \cdots, 900, 1000\rangle$</li>
<li>训练集总共包括 $T = 40 \times n = 500$ 即 500个<code>trade print</code>吗？所以一个epoch囊括了一个小时左右的市场订单信息
<img src="transition_state.png" alt="" title="如何将LOB观测量和订单信息通过auto-encoder转换|||80"></li>
</ul>
</li>
<li>对比：1. <code>momentum</code>: 通过比较$\text{mid}_{t-T}$和$\text{mid}_{t}$决定是否下一阶段direction, 至于quantity多少本文没有提及；2. <code>classifier</code>：通过$s_t$预测$\text{mid}_{t+1}$也没有说明action；3. <code>greedy</code>: 假设下一个阶段的LOB是已知的，只优化$ob_{t+1}$（而不是$ob_{t+1:t+T}$的所有信息都知道）已知条件下的策略，这样就只用优化<code>21 discretized actions</code>.</li>
</ul>

  </div>
</div>

<h2 id="reference">Reference</h2>
<p>[1] Avellaneda, M., &amp; Stoikov, S. (2008). <a href="https://www.tandfonline.com/doi/full/10.1080/14697680701381228">High-frequency trading in a limit order book</a>. Quantitative Finance, 8(3), 217-224.</p>
	

<a href="#" id="back-to-top" class="back-to-top" style="display: inline;">Back to Top</a>

<script>
  var link = document.getElementById("back-to-top");
  var amountScrolled = 1000;

  window.addEventListener('scroll', function(e) {
      if ( window.pageYOffset > amountScrolled ) {
          link.classList.add('show');
      } else {
          link.className = 'back-to-top';
      }
  });

<!-- Scrolls to Top -->
  link.addEventListener('click', function(e) {
      e.preventDefault();

      var distance = 0 - window.pageYOffset;
      var increments = distance/(500/16);
      function animateScroll() {
          window.scrollBy(0, increments);
          if (window.pageYOffset <= document.body.offsetTop) {
              clearInterval(runAnimation);
          }
      };
      
      var runAnimation = setInterval(animateScroll, 16);
  });
</script>

<style>
  .back-to-top {
    background: #111;
    color: #fefefe;
    opacity: 0;
    transition: opacity .6s ease-in-out;
    z-index: 999;
    position: fixed;
    right: 30px;
    bottom: 30px;
    width: auto;
    height: auto;
    box-sizing: border-box;
    border-radius: 0.4em;
    word-wrap: normal !important;
  }

  a.back-to-top {
    font-weight: bold;
    letter-spacing: 0px;
    font-size: 0.75rem;
    font-family: inherit;
    text-transform: uppercase;
    text-align: center;
    padding: 0.65em 0.55em;
  }

  .back-to-top:hover, .back-to-top:focus, .back-to-top:visited {
    color: #fefefe;
    border-bottom: none;
  }

  .back-to-top.show {
    opacity: 1;
  }
</style>


  </main>
  <div id="disqus-container">
  
</div>


          <footer role="contentinfo">
  <div>
    <label for="themer">
      dark theme: <input type="checkbox" id="themer" class="vh">
      <span aria-hidden="true"></span>
    </label>
  </div>
  
    © 2020
  
</footer>

        </div>
      </div>
    </div>
    <script src="https://dennislblog.github.io/cnblog/js/prism.js"></script>
<script src="https://dennislblog.github.io/cnblog/js/dom-scripts.js"></script>
<script src='https://kit.fontawesome.com/a076d05399.js'></script>
    
  

  </body>
</html>
