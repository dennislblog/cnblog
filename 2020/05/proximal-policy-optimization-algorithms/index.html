<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <meta name="generator" content="Hugo 0.69.0" />
  <link rel="canonical" href="https://dennislblog.github.io/cnblog/2020/05/proximal-policy-optimization-algorithms/">

  

  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#000000">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="theme-color" content="#ffffff">

  <link rel="stylesheet" href="https://dennislblog.github.io/cnblog/css/prism.css" media="none" onload="this.media='all';">

  
  
  <link rel="stylesheet" type="text/css" href="https://dennislblog.github.io/cnblog/css/styles.css">

  <style id="inverter" media="none">
    .intro-and-nav, .main-and-footer { filter: invert(100%) }
    * { background-color: inherit }
    img:not([src*=".svg"]), .colors, iframe, .demo-container { filter: invert(100%) }
  </style>

  
  
  <title>PPO Algorithm | My Site Title</title>
  

<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"]},
    },
    "HTML-CSS": {
    	scale: 80,
    	styles: {
    		".MathJax": {color: "rgb(91, 58, 17)",}
    	},
    },
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>




</head>

  <body>
    <a href="#main">skip to content</a>
    <svg style="display: none">
  <symbol id="bookmark" viewBox="0 0 40 50">
   <g transform="translate(2266 3206.2)">
    <path style="stroke:currentColor;stroke-width:3.2637;fill:none" d="m-2262.2-3203.4-.2331 42.195 16.319-16.318 16.318 16.318.2331-42.428z"/>
   </g>
  </symbol>

  <symbol id="w3c" viewBox="0 0 127.09899 67.763">
   <text font-size="83" style="font-size:83px;font-family:Trebuchet;letter-spacing:-12;fill-opacity:0" letter-spacing="-12" y="67.609352" x="-26.782778">W3C</text>
   <text font-size="83" style="font-size:83px;font-weight:bold;font-family:Trebuchet;fill-opacity:0" y="67.609352" x="153.21722" font-weight="bold">SVG</text>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m33.695.377 12.062 41.016 12.067-41.016h8.731l-19.968 67.386h-.831l-12.48-41.759-12.479 41.759h-.832l-19.965-67.386h8.736l12.061 41.016 8.154-27.618-3.993-13.397h8.737z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m91.355 46.132c0 6.104-1.624 11.234-4.862 15.394-3.248 4.158-7.45 6.237-12.607 6.237-3.882 0-7.263-1.238-10.148-3.702-2.885-2.47-5.02-5.812-6.406-10.022l6.82-2.829c1.001 2.552 2.317 4.562 3.953 6.028 1.636 1.469 3.56 2.207 5.781 2.207 2.329 0 4.3-1.306 5.909-3.911 1.609-2.606 2.411-5.738 2.411-9.401 0-4.049-.861-7.179-2.582-9.399-1.995-2.604-5.129-3.912-9.397-3.912h-3.327v-3.991l11.646-20.133h-14.062l-3.911 6.655h-2.493v-14.976h32.441v4.075l-12.31 21.217c4.324 1.385 7.596 3.911 9.815 7.571 2.22 3.659 3.329 7.953 3.329 12.892z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.21 0 1.414 8.6-5.008 9.583s-1.924-4.064-5.117-6.314c-2.693-1.899-4.447-2.309-7.186-1.746-3.527.73-7.516 4.938-9.258 10.13-2.084 6.21-2.104 9.218-2.178 11.978-.115 4.428.58 7.043.58 7.043s-3.04-5.626-3.011-13.866c.018-5.882.947-11.218 3.666-16.479 2.404-4.627 5.954-7.404 9.114-7.728 3.264-.343 5.848 1.229 7.841 2.938 2.089 1.788 4.213 5.698 4.213 5.698l4.94-9.837z"/>
   <path style="fill:currentColor;image-rendering:optimizeQuality;shape-rendering:geometricPrecision" d="m125.82 48.674s-2.208 3.957-3.589 5.48c-1.379 1.524-3.849 4.209-6.896 5.555-3.049 1.343-4.646 1.598-7.661 1.306-3.01-.29-5.807-2.032-6.786-2.764-.979-.722-3.486-2.864-4.897-4.854-1.42-2-3.634-5.995-3.634-5.995s1.233 4.001 2.007 5.699c.442.977 1.81 3.965 3.749 6.572 1.805 2.425 5.315 6.604 10.652 7.545 5.336.945 9.002-1.449 9.907-2.031.907-.578 2.819-2.178 4.032-3.475 1.264-1.351 2.459-3.079 3.116-4.108.487-.758 1.276-2.286 1.276-2.286l-1.276-6.644z"/>
  </symbol>

  <symbol id="tag" viewBox="0 0 177.16535 177.16535">
    <g transform="translate(0 -875.2)">
     <path style="fill-rule:evenodd;stroke-width:0;fill:currentColor" d="m159.9 894.3-68.79 8.5872-75.42 77.336 61.931 60.397 75.429-76.565 6.8495-69.755zm-31.412 31.835a10.813 10.813 0 0 1 1.8443 2.247 10.813 10.813 0 0 1 -3.5174 14.872l-.0445.0275a10.813 10.813 0 0 1 -14.86 -3.5714 10.813 10.813 0 0 1 3.5563 -14.863 10.813 10.813 0 0 1 13.022 1.2884z"/>
    </g>
  </symbol>

  <symbol id="balloon" viewBox="0 0 141.73228 177.16535">
   <g transform="translate(0 -875.2)">
    <g>
     <path style="fill:currentColor" d="m68.156 882.83-.88753 1.4269c-4.9564 7.9666-6.3764 17.321-5.6731 37.378.36584 10.437 1.1246 23.51 1.6874 29.062.38895 3.8372 3.8278 32.454 4.6105 38.459 4.6694-.24176 9.2946.2879 14.377 1.481 1.2359-3.2937 5.2496-13.088 8.886-21.623 6.249-14.668 8.4128-21.264 10.253-31.252 1.2464-6.7626 1.6341-12.156 1.4204-19.764-.36325-12.93-2.1234-19.487-6.9377-25.843-2.0833-2.7507-6.9865-7.6112-7.9127-7.8436-.79716-.20019-6.6946-1.0922-6.7755-1.0248-.02213.0182-5.0006-.41858-7.5248-.22808l-2.149-.22808h-3.3738z"/>
     <path style="fill:currentColor" d="m61.915 883.28-3.2484.4497c-1.7863.24724-3.5182.53481-3.8494.63994-2.4751.33811-4.7267.86957-6.7777 1.5696-.28598 0-1.0254.20146-2.3695.58589-5.0418 1.4418-6.6374 2.2604-8.2567 4.2364-6.281 7.6657-11.457 18.43-12.932 26.891-1.4667 8.4111.71353 22.583 5.0764 32.996 3.8064 9.0852 13.569 25.149 22.801 37.517 1.3741 1.841 2.1708 2.9286 2.4712 3.5792 3.5437-1.1699 6.8496-1.9336 10.082-2.3263-1.3569-5.7831-4.6968-21.86-6.8361-33.002-.92884-4.8368-2.4692-14.322-3.2452-19.991-.68557-5.0083-.77707-6.9534-.74159-15.791.04316-10.803.41822-16.162 1.5026-21.503 1.4593-5.9026 3.3494-11.077 6.3247-15.852z"/>
     <path style="fill:currentColor" d="m94.499 885.78c-.10214-.0109-.13691 0-.0907.0409.16033.13489 1.329 1.0675 2.5976 2.0723 6.7003 5.307 11.273 14.568 12.658 25.638.52519 4.1949.24765 14.361-.5059 18.523-2.4775 13.684-9.7807 32.345-20.944 53.519l-3.0559 5.7971c2.8082.76579 5.7915 1.727 8.9926 2.8441 11.562-11.691 18.349-19.678 24.129-28.394 7.8992-11.913 11.132-20.234 12.24-31.518.98442-10.02-1.5579-20.876-6.7799-28.959-.2758-.4269-.57803-.86856-.89617-1.3166-3.247-6.13-9.752-12.053-21.264-16.131-2.3687-.86369-6.3657-2.0433-7.0802-2.1166z"/>
     <path style="fill:currentColor" d="m32.52 892.22c-.20090-.13016-1.4606.81389-3.9132 2.7457-11.486 9.0476-17.632 24.186-16.078 39.61.79699 7.9138 2.4066 13.505 5.9184 20.562 5.8577 11.77 14.749 23.219 30.087 38.74.05838.059.12188.1244.18052.1838 1.3166-.5556 2.5965-1.0618 3.8429-1.5199-.66408-.32448-1.4608-1.3297-3.8116-4.4602-5.0951-6.785-8.7512-11.962-13.051-18.486-5.1379-7.7948-5.0097-7.5894-8.0586-13.054-6.2097-11.13-8.2674-17.725-8.6014-27.563-.21552-6.3494.13041-9.2733 1.775-14.987 2.1832-7.5849 3.9273-10.986 9.2693-18.07 1.7839-2.3656 2.6418-3.57 2.4409-3.7003z"/>
     <path style="fill:currentColor" d="m69.133 992.37c-6.2405.0309-12.635.76718-19.554 2.5706 4.6956 4.7759 9.935 10.258 12.05 12.625l4.1272 4.6202h11.493l3.964-4.4516c2.0962-2.3541 7.4804-7.9845 12.201-12.768-8.378-1.4975-16.207-2.6353-24.281-2.5955z"/>
     <rect style="stroke-width:0;fill:currentColor" ry="2.0328" height="27.746" width="22.766" y="1017.7" x="60.201"/>
    </g>
   </g>
  </symbol>

  <symbol id="info" viewBox="0 0 41.667 41.667">
   <g transform="translate(-37.035 -1004.6)">
    <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m76.25 1030.2a18.968 18.968 0 0 1 -23.037 13.709 18.968 18.968 0 0 1 -13.738 -23.019 18.968 18.968 0 0 1 23.001 -13.768 18.968 18.968 0 0 1 13.798 22.984"/>
    <g transform="matrix(1.1146 0 0 1.1146 -26.276 -124.92)">
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:3.728;fill:none" d="m75.491 1039.5v-8.7472"/>
     <path style="stroke-width:0;fill:currentColor" transform="scale(-1)" d="m-73.193-1024.5a2.3719 2.3719 0 0 1 -2.8807 1.7142 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
   </g>
  </symbol>

  <symbol id="warning" viewBox="0 0 48.430474 41.646302">
    <g transform="translate(-1.1273 -1010.2)">
     <path style="stroke-linejoin:round;stroke:currentColor;stroke-linecap:round;stroke-width:4.151;fill:none" d="m25.343 1012.3-22.14 37.496h44.28z"/>
     <path style="stroke:currentColor;stroke-linecap:round;stroke-width:4.1512;fill:none" d="m25.54 1027.7v8.7472"/>
     <path style="stroke-width:0;fill:currentColor" d="m27.839 1042.8a2.3719 2.3719 0 0 1 -2.8807 1.7143 2.3719 2.3719 0 0 1 -1.718 -2.8785 2.3719 2.3719 0 0 1 2.8763 -1.7217 2.3719 2.3719 0 0 1 1.7254 2.8741"/>
    </g>
  </symbol>

  <symbol id="menu" viewBox="0 0 50 50">
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="0" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="20" x="0"/>
     <rect style="stroke-width:0;fill:currentColor" height="10" width="50" y="40" x="0"/>
   </symbol>

   <symbol id="link" viewBox="0 0 50 50">
    <g transform="translate(0 -1002.4)">
     <g transform="matrix(.095670 0 0 .095670 2.3233 1004.9)">
      <g>
       <path style="stroke-width:0;fill:currentColor" d="m452.84 192.9-128.65 128.65c-35.535 35.54-93.108 35.54-128.65 0l-42.881-42.886 42.881-42.876 42.884 42.876c11.845 11.822 31.064 11.846 42.886 0l128.64-128.64c11.816-11.831 11.816-31.066 0-42.9l-42.881-42.881c-11.822-11.814-31.064-11.814-42.887 0l-45.928 45.936c-21.292-12.531-45.491-17.905-69.449-16.291l72.501-72.526c35.535-35.521 93.136-35.521 128.64 0l42.886 42.881c35.535 35.523 35.535 93.141-.001 128.66zm-254.28 168.51-45.903 45.9c-11.845 11.846-31.064 11.817-42.881 0l-42.884-42.881c-11.845-11.821-11.845-31.041 0-42.886l128.65-128.65c11.819-11.814 31.069-11.814 42.884 0l42.886 42.886 42.876-42.886-42.876-42.881c-35.54-35.521-93.113-35.521-128.65 0l-128.65 128.64c-35.538 35.545-35.538 93.146 0 128.65l42.883 42.882c35.51 35.54 93.11 35.54 128.65 0l72.496-72.499c-23.956 1.597-48.092-3.784-69.474-16.283z"/>
      </g>
     </g>
    </g>
  </symbol>

  <symbol id="doc" viewBox="0 0 35 45">
   <g transform="translate(-147.53 -539.83)">
    <path style="stroke:currentColor;stroke-width:2.4501;fill:none" d="m149.38 542.67v39.194h31.354v-39.194z"/>
    <g style="stroke-width:25" transform="matrix(.098003 0 0 .098003 133.69 525.96)">
     <path d="m220 252.36h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path style="stroke:currentColor;stroke-width:25;fill:none" d="m220 409.95h200"/>
     <path d="m220 488.74h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
     <path d="m220 331.15h200" style="stroke:currentColor;stroke-width:25;fill:none"/>
    </g>
   </g>
 </symbol>

 <symbol id="tick" viewBox="0 0 177.16535 177.16535">
  <g transform="translate(0 -875.2)">
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="155" width="40" y="702.99" x="556.82"/>
   <rect style="stroke-width:0;fill:currentColor" transform="rotate(30)" height="40" width="90.404" y="817.99" x="506.42"/>
  </g>
 </symbol>
</svg>

    <div class="wrapper">
      <header class="intro-and-nav" role="banner">
  <div>
    <div class="intro">
      <a class="logo" href="/" aria-label="My Site Title home page">
        <img src="https://dennislblog.github.io/cnblog/pics/logo.svg" alt="">
      </a>
      <p class="library-desc">
        
        Hello, you all
        
      </p>
    </div>
    <nav id="patterns-nav" class="patterns" role="navigation">
  <h2 class="vh">Main navigation</h2>
  <button id="menu-button" aria-expanded="false">
    <svg viewBox="0 0 50 50" aria-hidden="true" focusable="false">
      <use xlink:href="#menu"></use>
    </svg>
    Menu
  </button>
  
  <ul id="patterns-list">
  
    <li class="pattern">
      
      
      
      
      <a href="/cnblog/post/" aria-current="page">
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Blog</span>
      </a>
    </li>
  
    <li class="pattern">
      
      
      
      
      <a href="/cnblog/tags/" >
        <svg class="bookmark-icon" aria-hidden="true" focusable="false" viewBox="0 0 40 50">
          <use xlink:href="#bookmark"></use>
        </svg>
        <span class="text">Tags</span>
      </a>
    </li>
  
  </ul>
</nav>
    
    



  <hr>
  <nav class="patterns" aria-labelledby="toc-heading">
    <h4 id="toc-heading" style="margin-bottom: 1em">Table of contents</h4> 
    <ol>
      
        
        <li class="toc-h2">
          
          
          
          
          <a href="#policy-gradient" style="padding-left: 0rem">
            Policy Gradient
          </a>
        </li>
      
        
        <li class="toc-h2">
          
          
          
          
          <a href="#trust-region-policy-optimization" style="padding-left: 0rem">
            Trust Region Policy Optimization
          </a>
        </li>
      
        
        <li class="toc-h2">
          
          
          
          
          <a href="#proximal-policy-optimization" style="padding-left: 0rem">
            Proximal Policy Optimization
          </a>
        </li>
      
        
        <li class="toc-h2">
          
          
          
          
          <a href="#reference" style="padding-left: 0rem">
            Reference
          </a>
        </li>
      
    </ol>
  </nav>





    
  </div>
</header>
      <div class="main-and-footer">
        <div>
          
  <main id="main">
    <h1>
      <svg class="bookmark-icon" aria-hidden="true" viewBox="0 0 40 50" focusable="false">
        <use xlink:href="#bookmark"></use>
      </svg>
      PPO Algorithm
    </h1>

    <div class="date">
      
      
      <strong aria-hidden="true">Publish date: </strong>Thursday, May 28, 2020
      
        
      
    </div>

    
      <div class="tags">
        <strong aria-hidden="true">Tags: </strong>
        <ul aria-label="tags">
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              
              <a href="https://dennislblog.github.io/cnblog/tags/rl/">RL</a>
            </li>
          
            <li>
              <svg class="tag-icon" aria-hidden="true" viewBox="0 0 177.16535 177.16535" focusable="false">
                <use xlink:href="#tag"></use>
              </svg>
              
              <a href="https://dennislblog.github.io/cnblog/tags/review/">Review</a>
            </li>
          
        </ul>
      </div>
    
    <p>summaries on <a href="https://arxiv.org/abs/1707.06347">proximal policy optimization</a>(Schulman, et al., 2017)</p>
<p><figure role="group">
    <a href=" https://i.postimg.cc/Pq7BspYD/image.png" class="img-link">
        <img src="https://i.postimg.cc/Pq7BspYD/image.png" width="80%" title="Stochastic policies">
    </a>
    <figcaption>
        连续控制问题, 来自Katerina Fragkiadaki课件&lt;Natural Policy Gradients, TRPO, PPO&gt;
    </figcaption>
</figure>
</figure></p>
<h2 id="policy-gradient">Policy Gradient</h2>


<aside aria-label="note" class="note">
  <div>
    <svg class="sign" aria-hidden="true" viewBox="0 0 41.667306 41.66729" focusable="false">
      <use xlink:href="#info"></use>
    </svg>
    
我一直有一个疑问, 怎么优化函数, 使得函数的输出 action 更靠近最优 action,但是其实我们并没有最优 action, 因此怎么去定义函数的 loss(y - f(x)) ? 
    <br>
    <ul>
<li><code>policy gradient</code>: 如果某一个动作得到<code>reward</code>多, 那么我们就使其出现的概率增大</li>
<li><code>log likelihood</code>: $\log\pi(a|s,\theta)$, 构造<code>loss function</code>$=\sum \log\pi(a|s,\theta) f(s,a)$, 这个$f(s,a)$如果是<code>reward</code>的话, 我们希望$f(s,a)$大的<code>(s,a)</code>的<code>log-likelihood</code>尽可能更大一些</li>
</ul>

  </div>
</aside>



<aside aria-label="note" class="note">
  <div>
    <svg class="sign" aria-hidden="true" viewBox="0 0 41.667306 41.66729" focusable="false">
      <use xlink:href="#info"></use>
    </svg>
    
为什么是log-likelihood而不是直接输出likelihood 
    <br>
    <ul>
<li>首先最大化<code>log-likelihood</code>等同于最大化<code>likelihood</code></li>
<li>其次$\ln(ab) = \ln(a) + \ln(b)$这个性质很有帮助, 简化运算</li>
<li>另外还有一个非常有用的性质, 就是当我们把期望收益作为目标优化时, 等同于对<code>log-likelihood</code>求导
$$\nabla_\theta\mathbb{E}_{(s,a)\sim\tau}[f(s,a)]\equiv \mathbb{E}_{(s,a)\sim\tau}\left[f(s,a)\nabla_\theta \log \pi(a|s,\theta) \right]$$</li>
</ul>

  </div>
</aside>

<!-- 求那个期望$\mathbb{E}_{(s,a)\sim\tau}$怎么求? 我们 -->


<aside aria-label="note" class="note">
  <div>
    <svg class="sign" aria-hidden="true" viewBox="0 0 41.667306 41.66729" focusable="false">
      <use xlink:href="#info"></use>
    </svg>
    
怎样的f(s,a)是一个好的策略评价指标呢? 
    <br>
    <ul>
<li>在<code>(monte carlo) vanilla reinforce</code>算法中，是收集整个<code>episode</code>的奖励(比如输赢)来评价<code>episode</code>回合中的每一个步骤的好坏，但其实整体奖励不高不一定代表没一个步骤都不好，因此出现了很多<code>heuristic</code>来直接评价f(s,a)，比如用<code>Q(s,a), V(s,a) 和 A(s,a)</code>等。A2C就是希望设计一个函数，直接对每一个步骤进行评价，来反馈给原策略函数</li>
</ul>

  </div>
</aside>

<h2 id="trust-region-policy-optimization">Trust Region Policy Optimization</h2>
<blockquote>
<p>为了解决一个问题：策略参数更新时选择多大的步长合适</p>
<ul>
<li>步长太大， 可能更新后策略变差，接着用这个差的策略去取样，导致后面的更新越来越差 (更恐怖的是山路急转弯，直接掉下去!)。步长太小，可能会收集重复/相似的经验，学习效率低下
<ul>
<li>思想一：如果要最大化某个目标，先去最大化这个目标的下界 (Minorize-Maximization)</li>
<li>思想二：当我们确定了<code>gradient direction</code>后，要保证选取合适的步长，使得最后参数空间仍然在<code>trust region</code>里(measured by <strong>KL divergence between old and updated policy</strong>)</li>
</ul>
</li>
</ul>
</blockquote>
<p><figure role="group">
    <a href=" https://www.52coding.com.cn/images/trpo.svg" class="img-link">
        <img src="https://www.52coding.com.cn/images/trpo.svg" width="80%" title="TRPO算法">
    </a>
    <figcaption>
        算法: TRPO = NPG +Linesearch+monotonic improvement theorem!
    </figcaption>
</figure>
</figure></p>
<div class="expandable-section">
  
    <h3>
  
    <button aria-expanded="false" data-expands="js-expandable-d95aa45bd17650ee0bb14f9f0ea9b250">
      <span class="expandable-label">详细算法证明</span>
      <svg aria-hidden="true" focusable="false" viewBox="0 0 70.866142 70.866141">
        <g transform="translate(0 -981.5)">
          <rect style="stroke-width:0;fill:currentColor" ry="5" height="60" width="9.8985" y="987.36" x="30.051" class="up-strut" />
          <rect style="stroke-width:0;fill:currentColor" ry="5" height="10" width="60" y="1012.4" x="5"/>
        </g>
      </svg>
    </button>
  
    </h3>
  
  <div id="js-expandable-d95aa45bd17650ee0bb14f9f0ea9b250" hidden>
    <p>首先定义$\rho_\pi(s)$为<code>discounted visitation frequency</code>
$$\rho_\pi(s) = \Pr(s_0=s)+\gamma \Pr(s_1=s) + \gamma^2 \Pr(s_2=s)+&hellip;$$
我们希望更新后的策略，能够获得更大的<code>advantage value</code> $A_\pi(s, a) = Q_\pi(s, a) - V_\pi(s)$，即 (下面式子怎么证明的，为什么<code>advantage value</code>包含<code>old policy</code>)
$$
\begin{align}
\eta(\pi&rsquo;)&amp;=\eta(\pi)+\mathbb{E}_{s_0, a_0, &hellip;\sim\color{red}{\pi&rsquo;}}[\sum_{t=0}^\infty\gamma^tA_\pi(s_t,a_t)]\cr
&amp;= \eta(\pi)+\sum_s\color{red}{\rho_{\pi&rsquo;}(s)}\sum_a\pi&rsquo;(a|s)A_\pi(s, a)
\end{align}
$$
根据上面的式子，只需要每个状态下的期望优势非负 $\sum_a\pi&rsquo;(a|s)A_\pi(s, a)&gt;0$ 就可以保证<code>new policy</code>产生更高的奖励 $\eta(\pi&rsquo;)=\mathbb{E}_{s_0, a_0, &hellip;\sim\pi&rsquo;}[\sum_{t=0}^\infty\gamma^tr(s_t)]$。但是最大的问题在于$\rho_{\pi&rsquo;}(s)$不好估计呀~(需要统计所有潜在新策略的访问频率)，因此退而求其次，引入<code>surrogate advantage</code> (可以被理解为<code>using importance sampling to est. the advantage function</code>)
$$\begin{align}L_\pi(\pi&rsquo;)&amp;=\sum_s\color{red}{\rho_\pi(s)}\sum_a\pi&rsquo;(a|s)A_\pi(s, a)\cr
&amp;= \mathbb{E}_{s,a\sim\pi}\left[\frac{\pi&rsquo;(a|s)}{\pi(a|s)}A_\pi(s,a)\right]
\end{align}$$
其中$a\sim \pi, s \sim d^\pi(s) = (1-\gamma)\sum_{t=0}^\infty \gamma^t \Pr(s_t=s|\pi)$, <code>the discounted state visitation frequency</code> 经过一系列推导，得到新策略目标的下界，即在新策略下，那些在老策略下奖励高的<code>(s,a)</code>访问概率更大，同时限制新旧策略的差距
$$\eta(\pi&rsquo;)≥L_\pi(\pi&rsquo;)-C\cdot D_{KL}^\max(\pi, \pi&rsquo;), \text{ where } C=\frac{4\epsilon\gamma}{(1-\gamma)^2}$$
<img src="https://i.postimg.cc/pd4gn8wT/image.png" alt="" title="通过优化下界不断逼近最优"></p>
<p>TRPO算法做了如下几点改变 (deep network 计算需要)</p>
<ol>
<li>首先是$D_{KL}^\max(\pi, \pi&rsquo;)$ relax成了期望距离 $\bar{D}_{KL}(\pi&rsquo; || \pi)=\mathbb{E}_{s\sim\pi}[D_{KL}(\pi&rsquo;(\cdot|s)||\pi(\cdot|s))]$</li>
<li>即使是期望距离也不是很好求，因此用泰勒展开近似求解 (这里用$\pi_\theta$表征旧策略，用$\pi_{\theta&rsquo;}$表征新策略)</li>
</ol>
<ul>
<li>$L(\theta, \theta&rsquo;) \approx g^T(\theta&rsquo;-\theta)$ 其中$g$就是<code>surrogate advantage</code>对$\theta'$的梯度在$\theta'=\theta$时的值 (i.e., $g = \triangledown_{\theta&rsquo;} J(\pi_{\theta&rsquo;})|_{\theta}$)</li>
<li>$\bar{D}_{KL}(\theta'||\theta)\approx \frac{1}{2}(\theta&rsquo;-\theta)^TH(\theta&rsquo;-\theta)$, H是关于$\theta'$的二次<code>Hessian Matrix</code></li>
</ul>
<ol start="3">
<li>用<code>hard constraint</code> $\bar{D}_{KL}(\theta'||\theta) \leq \delta$ 而不是 <code>KL penalized objective</code> 因为$\delta$ 相比 $C \propto \frac{\gamma}{(1-\gamma)^2}$ 更好调试</li>
</ol>
<p>于是我们通过解$\max L(\theta&rsquo;,\theta) \text{ s.t. } \bar{D}(\theta'||\theta) \leq \delta$，得到<code>Natural Policy Gradient (NPG)</code>的更新公式<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>
$$\theta_{k+1}=\theta_k+\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g$$
由于泰勒近似引入了误差，上式的解可能不满足约束$\bar{D}_{KL}(\theta'||\theta)≤\delta$，所以 TRPO 还增加了一个线性搜索(backtracking line search)
$$\theta_{k+1}=\theta_k+\alpha^j\sqrt{\frac{2\delta}{g^TH^{-1}g}}H^{-1}g$$
其中$\alpha \in (0,1)$是<code>backtracking coefficient</code><sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>。另外为了避免存储$H^{-1}$，采用<code>conjugate gradient</code>来计算$g^TH^{-1}g$<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>，更新步骤为
$$\theta_{k+1}=\theta_k+\alpha^j\sqrt{\frac{2\delta}{\hat{x}^TH\hat{x}}}\hat{x} \quad \text{ where } \hat{x}\approx H^{-1}g$$
<img src="https://i.postimg.cc/vB4vKng6/image.png" alt="TRPO算法总结" title="TRPO算法步骤"></p>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p><code>gradient descent</code>很受<code>parameter space</code>的影响， 当优化问题的两个变量坐标轴尺度差异大时，统一的学习率会因为震荡过大而收敛过慢，自然梯度由于考虑到了<code>curvature (2nd order derivative) of log-prob of policy</code>会对模型参数变化更友好? <br>1. 随机梯度下降 $m \leftarrow \eta \nabla_\theta J(\theta) \qquad \theta&rsquo; \leftarrow \theta - m$ <br>2. <code>Momentum</code>动量法 $m \leftarrow \gamma + \eta \nabla_\theta J(\theta) \qquad \theta&rsquo; \leftarrow \theta - m$, $\gamma$ 代表对历史梯度的记忆，$\gamma$越大越难修正梯度方向 <br>3. <code>Nesterov</code>牛顿动量法 $m \leftarrow \gamma m + \eta \nabla_\theta J(\theta + \beta m) \qquad \theta&rsquo; \leftarrow \theta - m$ 前瞻梯度更新 <br>4. <code>NGD</code>自然梯度法 $m \leftarrow \frac{1}{\lambda}F^{-1}_\theta\nabla_\theta J(\theta) \qquad \theta&rsquo; \leftarrow \theta - m$ 其中$F = \mathbb{E}_{J(\theta)}\left[\nabla_\theta \log J(\theta) (\nabla_\theta \log J(\theta))^T \right]$ 代表关于现有模型的<code>Fisher</code>信息矩阵，可以帮助理解新旧模型之间的距离: $$\bar{D}_{KL}(\theta'||\theta)\approx \frac{1}{2}(\theta&rsquo;-\theta)^TF(\theta&rsquo;-\theta)$$ <a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>也就是说先尝试最激进的更新, 如果违反了<code>KL Constraint</code>的话，就把更新<code>step</code>再乘以一个$\alpha$，一直到约束条件满足为止 <a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>普通<code>gradient descent</code>是遵循最大梯度原则，因此会由于噪音等因素往返重复，而<code>conjugate gradient</code>则要求当前梯度方向与之前所有梯度方向垂直 (见下图) <br><img src="https://i.postimg.cc/J7cpKvpZ/image.png" alt=""> <a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

  </div>
</div>

<h2 id="proximal-policy-optimization">Proximal Policy Optimization</h2>
<h2 id="reference">Reference</h2>
<p><a href="https://www.youtube.com/watch?v=OAKAZhFmYoI">DRL Lecture 2: Proximal Policy Optimization (PPO)</a></p>
<p><a href="https://www.youtube.com/watch?v=CKaN5PgkSBc">TRPO (Trust Region Policy Optimization): In depth Research Paper Review</a></p>
<p><a href="https://stats.stackexchange.com/questions/341657/comaprsion-between-natural-gradient-descent-and-stochastic-gradient-descent">Comaprsion between Natural Gradient Descent and Stochastic Gradient Descent</a></p>
<p><a href="https://www.52coding.com.cn/2018/11/25/RL%20-%20PPO/">RL - Proximal Policy Optimization (PPO)</a></p>
	

<a href="#" id="back-to-top" class="back-to-top" style="display: inline;">Back to Top</a>

<script>
  var link = document.getElementById("back-to-top");
  var amountScrolled = 1000;

  window.addEventListener('scroll', function(e) {
      if ( window.pageYOffset > amountScrolled ) {
          link.classList.add('show');
      } else {
          link.className = 'back-to-top';
      }
  });

<!-- Scrolls to Top -->
  link.addEventListener('click', function(e) {
      e.preventDefault();

      var distance = 0 - window.pageYOffset;
      var increments = distance/(500/16);
      function animateScroll() {
          window.scrollBy(0, increments);
          if (window.pageYOffset <= document.body.offsetTop) {
              clearInterval(runAnimation);
          }
      };
      
      var runAnimation = setInterval(animateScroll, 16);
  });
</script>

<style>
  .back-to-top {
    background: #111;
    color: #fefefe;
    opacity: 0;
    transition: opacity .6s ease-in-out;
    z-index: 999;
    position: fixed;
    right: 30px;
    bottom: 30px;
    width: auto;
    height: auto;
    box-sizing: border-box;
    border-radius: 0.4em;
    word-wrap: normal !important;
  }

  a.back-to-top {
    font-weight: bold;
    letter-spacing: 0px;
    font-size: 0.75rem;
    font-family: inherit;
    text-transform: uppercase;
    text-align: center;
    padding: 0.65em 0.55em;
  }

  .back-to-top:hover, .back-to-top:focus, .back-to-top:visited {
    color: #fefefe;
    border-bottom: none;
  }

  .back-to-top.show {
    opacity: 1;
  }
</style>


  </main>
  <div id="disqus-container">
  
</div>


          <footer role="contentinfo">
  <div>
    <label for="themer">
      dark theme: <input type="checkbox" id="themer" class="vh">
      <span aria-hidden="true"></span>
    </label>
  </div>
  
    © 2020
  
</footer>

        </div>
      </div>
    </div>
    <script src="https://dennislblog.github.io/cnblog/js/prism.js"></script>
<script src="https://dennislblog.github.io/cnblog/js/dom-scripts.js"></script>
<script src='https://kit.fontawesome.com/a076d05399.js'></script>
    
  

  </body>
</html>
